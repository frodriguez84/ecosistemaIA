# Ecosistema Evolutivo IA

**Contexto General**

Est√°s trabajando en el desarrollo de una aplicaci√≥n completa de simulaci√≥n llamada **"Ecosistema Evolutivo IA"**, un entorno 2D donde criaturas aut√≥nomas aprenden a sobrevivir, adaptarse y evolucionar utilizando m√∫ltiples t√©cnicas de **Inteligencia Artificial**:

* **Algoritmos gen√©ticos** - Evoluci√≥n de poblaciones mediante selecci√≥n, cruce y mutaci√≥n
* **Redes neuronales artificiales** - Cerebro de los agentes que toma decisiones aut√≥nomas
* **Clustering no supervisado** - An√°lisis de comportamientos emergentes (K-Means, PCA)
* **Generaci√≥n procedural del entorno** - Mundo din√°mico con fortalezas, puzzle, recursos y obst√°culos

El proyecto est√° desarrollado en **Python 3.11+** y mantiene una **arquitectura modular, extensible y limpia**, siguiendo principios de dise√±o como:

* *Separation of Concerns*
* *Single Responsibility Principle*
* *Aprendizaje puro* - Sin gu√≠a codificada, los agentes aprenden 100% mediante su red neuronal
* Soporte para logging estructurado, reproducibilidad y ejecuci√≥n por fases

---

## üéØ Objetivo del Proyecto

Crear un **ecosistema digital autoevolutivo** donde criaturas simuladas:

1. **Perciben su entorno** - Sensores de distancia, energ√≠a, comida, obst√°culos, objetos del puzzle
2. **Toman decisiones mediante red neuronal** - 5 acciones: avanzar, girar izquierda/derecha, comer, golpear
3. **Son evaluadas por su desempe√±o** - Fitness basado en supervivencia, comida, exploraci√≥n, eficiencia y puzzle
4. **Se reproducen mediante algoritmo gen√©tico** - Selecci√≥n (meeting pool + elitismo), cruce, mutaci√≥n
5. **Dan lugar a nuevas generaciones** - Poblaciones m√°s adaptadas con comportamientos emergentes
6. **Son analizadas mediante clustering** - Identificaci√≥n de estrategias emergentes (Exploradores, Recolectores, Exitosos)

El resultado es visual, con una interfaz completa que muestra:

* El mapa procedural (pastos, √°rboles, fortalezas, agua, estanques)
* Criaturas movi√©ndose y actuando de forma aut√≥noma
* Sistema de puzzle (llaves, puertas, cofre)
* M√©tricas en tiempo real (generaci√≥n, fitness promedio, supervivientes, diversidad)

---

## ‚öôÔ∏è Stack Tecnol√≥gico

**Lenguaje:** Python 3.11+

**Librer√≠as Principales:**

* `pygame` ‚Üí Renderizado 2D, sprites y eventos
* `numpy` ‚Üí Operaciones matem√°ticas y arrays
* `scikit-learn` ‚Üí Clustering (K-Means), PCA, normalizaci√≥n
* `matplotlib` / `seaborn` ‚Üí Visualizaci√≥n de m√©tricas (opcional)
* `pandas` ‚Üí An√°lisis de datos (opcional)

**Entorno y herramientas:**

* Control de versiones con Git
* Requerimientos en `requirements.txt`
* Soporte para ejecuci√≥n local con render o modo headless (sin visualizaci√≥n)

---

## üß© Estructura del Proyecto

```
EcosistemaEvolutivo/
‚îú‚îÄ‚îÄ main.py                  # Punto de entrada principal
‚îú‚îÄ‚îÄ config.py               # Configuraci√≥n centralizada (SimulationConfig)
‚îú‚îÄ‚îÄ requirements.txt        # Dependencias del proyecto
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/             # Sistema de agentes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced_agent.py    # Clase principal AdvancedAgent
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ brain/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ mlp.py           # Red neuronal MLP (SimpleNeuralNetwork)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ world/              # Mundo y entorno
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ world.py       # Clase World (generaci√≥n procedural, fortalezas, puzzle)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ obstacles.py   # Obst√°culos (√°rboles, muros, agua, huts, etc.)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evolution/          # Algoritmo gen√©tico
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ genetic_algorithm.py  # Selecci√≥n, cruce, mutaci√≥n, evoluci√≥n
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analytics/          # An√°lisis y m√©tricas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # C√°lculo de m√©tricas generacionales
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clustering.py        # Clustering de comportamientos (K-Means, PCA)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ learning_monitor.py  # Monitoreo de aprendizaje y mejoras
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py            # Logging estructurado
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ui/                 # Interfaz de usuario
‚îÇ       ‚îú‚îÄ‚îÄ renderer.py     # Renderizado con pygame (sprites, part√≠culas)
‚îÇ       ‚îú‚îÄ‚îÄ stats.py        # Panel de estad√≠sticas
‚îÇ       ‚îî‚îÄ‚îÄ popup.py        # Popup de resumen generacional
‚îÇ
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îî‚îÄ‚îÄ sprites/            # Sprites del juego (agentes, objetos, entorno)
‚îÇ
‚îú‚îÄ‚îÄ logs/                   # Logs de m√©tricas y runtime
‚îú‚îÄ‚îÄ checkpoints/            # Checkpoints de generaciones (opcional)
‚îî‚îÄ‚îÄ results/                # Resultados y reportes (opcional)
```

---

## üß± Funcionalidades Implementadas (por m√≥dulos)

### 1. `src/agents/`

* **`advanced_agent.py`**: Clase `AdvancedAgent` con:

  * **Sensores**: 12 percepciones (distancia a comida, obst√°culos, objetos del puzzle, energ√≠a, direcci√≥n)
  * **Red neuronal**: MLP con 2 capas ocultas [24, 18] y 5 salidas (avanzar, girar L/R, comer, golpear)
  * **Sistema de fitness**: Supervivencia + comida + exploraci√≥n + evitar obst√°culos + anti-c√≠rculo + puzzle
  * **Acciones**: Movimiento, alimentaci√≥n, corte de √°rboles, interacci√≥n con puzzle
  * **Aprendizaje puro**: **100% aut√≥nomo** - Sin gu√≠a codificada, todo basado en decisiones de la red neuronal
* **`brain/mlp.py`**:

  * Red neuronal MLP (`SimpleNeuralNetwork`)
  * Funci√≥n de activaci√≥n: `tanh`
  * Operaciones: forward pass, mutaci√≥n gaussiana, cruce uniforme, clonaci√≥n

### 2. `src/world/`

* **`world.py`**: Clase `World` que genera y gestiona:

  * **Generaci√≥n procedural**: Pastos, √°rboles, muros, agua, estanques, per√≠metro
  * **Sistema de fortalezas**: Fortaleza peque√±a (4x5 tiles) y grande (6x6 tiles) con muros y puertas
  * **Sistema de puzzle**: Llaves rojas/doradas, puertas (wood/iron), cofre
  * **Recursos**: Comida (manzanas), √°rboles que pueden cortarse, huts destructibles
  * **Estanque m√≥vil**: Estanque que se regenera din√°micamente
* **`obstacles.py`**:

  * Tipos de obst√°culos: `tree`, `wall`, `water`, `hut`, `safe`
  * Efectos: p√©rdida de energ√≠a, reducci√≥n de velocidad, penalizaci√≥n de fitness (agua)
  * Sistema de corte: √Årboles y huts pueden cortarse con hacha (requiere acci√≥n `hit`)

### 3. `src/evolution/`

* **`genetic_algorithm.py`**: Algoritmo gen√©tico completo:
  * **Selecci√≥n**: Meeting pool (fracci√≥n superior por ranking) + elitismo opcional
  * **Cruce**: Uniforme entre genomas (pesos de red neuronal)
  * **Mutaci√≥n**: Gaussiana con tasa configurable (20%)
  * **Evoluci√≥n**: Genera nueva poblaci√≥n manteniendo mejores individuos

### 4. `src/analytics/`

* **`metrics.py`**: C√°lculo de m√©tricas generacionales:

  * Fitness promedio, m√°ximo, m√≠nimo
  * Diversidad gen√©tica
  * Supervivientes, edad promedio
  * Comida total consumida, distancia recorrida
* **`clustering.py`**: An√°lisis no supervisado:

  * **Features**: Fitness, comida, exploraci√≥n, supervivencia
  * **Normalizaci√≥n**: StandardScaler
  * **Reducci√≥n de dimensi√≥n**: PCA (2 componentes)
  * **Clustering**: K-Means (k=3) - Identifica "Exploradores", "Recolectores", "Exitosos"
  * Ejecuci√≥n cada 3 generaciones
* **`learning_monitor.py`**: Monitoreo de aprendizaje:

  * Detecta mejoras en comida (absoluta y relativa)
  * Detecta mejoras en fitness
  * Detecta cambios en diversidad gen√©tica
  * Reportes en consola con emojis
* **`logger.py`**: Logging estructurado a archivos

### 5. `src/ui/`

* **`renderer.py`**:

  * Renderizado con pygame
  * Sistema de sprites (personajes, objetos, entorno)
  * Sistema de part√≠culas para efectos visuales
  * Renderizado de agentes, mundo, HUD
* **`stats.py`**: Panel lateral de estad√≠sticas en tiempo real
* **`popup.py`**: Popup de resumen al final de cada generaci√≥n

### 6. `config.py`

* **`SimulationConfig`**: Clase con todos los par√°metros centralizados:
  * Poblaci√≥n, generaciones m√°ximas
  * Par√°metros del algoritmo gen√©tico
  * Arquitectura de red neuronal
  * Sistema de puzzle (recompensas, umbrales)
  * Sistema de agua (penalizaciones, energ√≠a)
  * Sistema de corte (√°rboles, huts)
  * Sistema adaptativo de tiempo (ticks por generaci√≥n)

---

## üß¨ Flujo General del Sistema

### Inicializaci√≥n

1. **Carga configuraci√≥n** desde `config.py` (SimulationConfig)
2. **Genera mundo procedural**:
   * Pastos, √°rboles, muros, agua, estanques
   * Fortalezas con puertas
   * Llaves, cofre (spawn seg√∫n generaci√≥n)
   * Per√≠metro del mapa
3. **Inicializa poblaci√≥n** con redes neuronales aleatorias

### Loop de Simulaci√≥n (por generaci√≥n)

1. **Cada tick (600-2000 ticks/generaci√≥n, adaptativo)**:

   * **Percepci√≥n**: Cada agente percibe su entorno (10 sensores)
   * **Decisi√≥n**: Red neuronal procesa percepciones ‚Üí 5 acciones
   * **Acci√≥n**: Agente ejecuta acciones (mover, girar, comer, golpear)
   * **Efectos del entorno**: Agua reduce energ√≠a/fitness, estanques restauran energ√≠a
   * **Interacciones**: Cortar √°rboles/huts, recoger llaves, abrir puertas, abrir cofre
   * **Muerte**: Si energ√≠a ‚â§ 0, agente muere
2. **Al finalizar la generaci√≥n**:

   * **C√°lculo de fitness**: Supervivencia + comida + exploraci√≥n + obst√°culos + anti-c√≠rculo + puzzle
   * **Ranking**: Ordenar agentes por fitness
   * **Clustering** (cada 3 generaciones): An√°lisis de comportamientos emergentes
   * **Algoritmo gen√©tico**:
     * Selecci√≥n (meeting pool)
     * Cruce uniforme
     * Mutaci√≥n gaussiana
     * Nueva poblaci√≥n
   * **Resumen**: Popup con estad√≠sticas de la generaci√≥n
   * **Reinicio**: Nuevo mundo, nueva poblaci√≥n evolucionada
3. **Condici√≥n de fin**:

   * Alcanzar `MAX_GENERATIONS` O
   * Abrir el cofre (objetivo principal)

---

## üß† Inteligencia Artificial (Detalles)

### Red Neuronal (Cerebro del Agente)

* **Tipo**: MLP (Multi-Layer Perceptron)
* **Arquitectura**:
  * Input: 10 sensores (distancia comida, obst√°culos, puzzle, energ√≠a, direcci√≥n)
  * Hidden: 2 capas [24, 18] neuronas
  * Output: 5 acciones (move_forward, turn_left, turn_right, eat, hit)
* **Activaci√≥n**: `tanh` (todas las capas)
* **Pesos**: Mutables por algoritmo gen√©tico
* **Aprendizaje**: **100% puro** - Sin gu√≠a codificada, decisiones completamente aut√≥nomas

### Algoritmo Gen√©tico

* **Poblaci√≥n**: 60 agentes (configurable)
* **Fitness**: Combinaci√≥n de:
  * Supervivencia (edad) - Cap: 23
  * Comida (sqrt) - Multiplicador: 12.0
  * Exploraci√≥n (log) - Multiplicador: 12.0, Cap: 15.0
  * Evitar obst√°culos - Multiplicador: 0.20
  * Anti-c√≠rculo (movimiento eficiente) - Multiplicador: 10.0
  * Puzzle (llaves, puertas, cofre) - Recompensas acumuladas
  * Penalizaciones (agua) - Sin l√≠mite, proporcional al tiempo
* **Selecci√≥n**: Meeting pool (90% superior) + elitismo opcional
* **Cruce**: Uniforme (90% tasa)
* **Mutaci√≥n**: Gaussiana (20% tasa)
* **Elitismo**: 0 (se puede configurar)

### Clustering (An√°lisis No Supervisado)

* **Features extra√≠das**:
  * Fitness total
  * Comida consumida
  * Distancia recorrida
  * Edad (supervivencia)
* **Preprocesamiento**:
  * Normalizaci√≥n: `StandardScaler`
  * Reducci√≥n de dimensi√≥n: `PCA` (2 componentes, 95% varianza)
* **Algoritmo**: K-Means con k=3
* **Clusters identificados**:
  * **Exploradores**: Alta exploraci√≥n, fitness medio
  * **Recolectores**: Alta comida, exploraci√≥n media
  * **Exitosos**: Alto fitness en todas las m√©tricas
* **Frecuencia**: Cada 3 generaciones

---

## üéÆ Sistema de Puzzle

El ecosistema incluye un sistema complejo de puzzle que los agentes deben aprender a resolver:

### Elementos del Puzzle

1. **Fortalezas**: Dos tipos (peque√±a 4x5, grande 6x6) con muros y puertas
2. **Llaves**:
   * **Red Key**: Aparece desde gen 5 (configurable), recompensa: 2 fitness
   * **Gold Key**: Dentro de fortaleza grande, recompensa: 10 fitness
3. **Puertas**:
   * **Wood Door**: Requiere 3 golpes, recompensa: 10 fitness
   * **Iron Door**: Requiere 3 golpes, recompensa: 20 fitness
4. **Cofre**: Objetivo final, recompensa: 85 fitness (alto para garantizar fitness 60+ al completar)

### Flujo del Puzzle

1. **Gen 1-4**: Agentes aprenden tareas b√°sicas (comer, sobrevivir, explorar)
2. **Gen 5+**: Aparecen llaves rojas en el mundo
3. **Aprendizaje emergente**: Los agentes aprenden a:
   * Recoger llave roja
   * Golpear puerta de madera (acci√≥n `hit` > 0.5)
   * Entrar a fortaleza peque√±a
   * Recoger llave dorada
   * Golpear puerta de hierro
   * Entrar a fortaleza grande
   * Abrir cofre
4. **Fin del juego**: Al abrir el cofre, la simulaci√≥n termina

### Mec√°nica de Aprendizaje

* **Sin gu√≠a codificada**: Los agentes descubren el puzzle mediante prueba y error
* **Recompensas**: Fitness positivo por cada acci√≥n exitosa del puzzle
* **Penalizaciones**: Agua penaliza fitness si pasan mucho tiempo en ella
* **Red neuronal**: Debe aprender a usar la acci√≥n `hit` cuando est√° cerca de puertas/√°rboles

---

## üåä Sistema de Agua

* **Efectos cuando un agente est√° en agua**:
  * **Energ√≠a**: Pierde 0.05 adicional por tick (adem√°s del consumo normal 0.05)
  * **Velocidad**: Reducida a 50% (SPEED_REDUCTION = 0.5)
  * **Fitness**: Penalizaci√≥n de 5 puntos por tick (sin l√≠mite m√°ximo)
* **Prop√≥sito**: Incentivar a los agentes a evitar el agua
* **Aprendizaje**: Los agentes evolucionan para minimizar tiempo en agua

---

## üå≥ Sistema de Corte

### √Årboles

* **Activaci√≥n**: Cuando quedan ‚â§30 manzanas en el mundo
* **Mec√°nica**: Agente con hacha puede cortar √°rbol (acci√≥n `hit` > 0.5)
* **Golpes requeridos**: 2
* **Recompensas**:
  * Fitness: +7
  * Comida: +20 manzanas generadas

### Huts (Casitas)

* **Activaci√≥n**: Cuando quedan ‚â§20 manzanas en el mundo
* **Mec√°nica**: Agente con hacha puede destruir hut (acci√≥n `hit` > 0.5)
* **Golpes requeridos**: 4
* **Recompensas**:
  * Fitness: +15
  * Comida: +30 manzanas generadas

---

## ‚öôÔ∏è Configuraci√≥n

El sistema usa `config.py` con la clase `SimulationConfig`:

```python
class SimulationConfig:
    # === SIMULACI√ìN ===
    POPULATION_SIZE = 60
    MAX_GENERATIONS = 50
    HEADLESS_MODE = True  # False para visualizaci√≥n
  
    # === ALGORITMO GEN√âTICO ===
    MUTATION_RATE = 0.20
    CROSSOVER_RATE = 0.90
    MEETING_POOL_FRACTION = 0.90
  
    # === RED NEURONAL ===
    INPUT_SIZE = 10
    HIDDEN_SIZE = [24, 18]
    OUTPUT_SIZE = 5  # move_forward, turn_left, turn_right, eat, hit
  
    # === SISTEMA DE PUZZLE ===
    RED_KEY_SPAWN_GEN = 5
    RED_KEY_REWARD = 2
    GOLD_KEY_REWARD = 10
    DOOR_OPEN_REWARD = 10
    DOOR_IRON_OPEN_REWARD = 20
    CHEST_REWARD = 85
  
    # === SISTEMA DE AGUA ===
    WATER_FITNESS_PENALTY = 5
    WATER_ENERGY_LOSS = 0.05
  
    # ... m√°s par√°metros
```

### Par√°metros Clave

* **`POPULATION_SIZE`**: Tama√±o de la poblaci√≥n (60)
* **`MAX_GENERATIONS`**: Generaciones m√°ximas (50)
* **`HEADLESS_MODE`**: `True` = sin render (r√°pido), `False` = con visualizaci√≥n
* **`MUTATION_RATE`**: Tasa de mutaci√≥n (0.20 = 20%)
* **`BASE_TICKS`**: Ticks iniciales por generaci√≥n (600)
* **`TICKS_INCREMENT_AMOUNT`**: Incremento de ticks (200)
* **`TICKS_INCREMENT_FREQUENCY`**: Cada cu√°ntas generaciones incrementar (2)

---

## üöÄ Instalaci√≥n y Uso

### Requisitos Previos

- Python 3.11+
- pip

### Instalaci√≥n

```bash
# 1. Clonar repositorio
git clone <repo-url>
cd EcosistemaEvolutivo

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Ejecutar simulaci√≥n
python main.py
```

### Modo de Ejecuci√≥n

El sistema puede ejecutarse en dos modos:

* **Modo visual** (`HEADLESS_MODE = False`):

  * Renderizado completo con pygame
  * Panel de estad√≠sticas en tiempo real
  * Popups de resumen generacional
  * M√°s lento pero visualmente informativo
* **Modo headless** (`HEADLESS_MODE = True`):

  * Sin renderizado visual
  * Solo output por consola
  * Mucho m√°s r√°pido para experimentos
  * Ideal para m√∫ltiples corridas

### Ejecutar Simulaci√≥n

```bash
# Modo visual
python main.py  # (configurar HEADLESS_MODE = False)

# Modo headless (r√°pido)
python main.py  # (configurar HEADLESS_MODE = True)
```

---

## üìä Resultados y An√°lisis

### M√©tricas Generadas

Cada corrida genera:

* **M√©tricas por consola**:

  * Fitness promedio/m√°ximo/m√≠nimo por generaci√≥n
  * Supervivientes, comida total, distancia recorrida
  * An√°lisis de clustering (cada 3 generaciones)
  * Monitoreo de aprendizaje (mejoras detectadas)
* **Logs estructurados** (opcional):

  * `logs/metrics/` - M√©tricas por generaci√≥n
  * `logs/runtime/` - Logs de runtime
* **Checkpoints** (opcional):

  * `checkpoints/` - Estados de generaciones

### An√°lisis de Comportamiento

El sistema incluye an√°lisis autom√°tico:

1. **Clustering**: Identifica 3 tipos de estrategias emergentes
2. **Monitoreo de aprendizaje**: Detecta mejoras en comida y fitness
3. **Diversidad gen√©tica**: Monitorea convergencia prematura

### Resultados Esperados

* **Evoluci√≥n natural**: Fitness promedio aumenta gradualmente
* **Comportamientos emergentes**: Agentes aprenden a comer, explorar, resolver puzzle
* **Diversidad**: M√∫ltiples estrategias coexisten
* **Resoluci√≥n del puzzle**: Agentes eventualmente abren el cofre (fitness 60+)
* **Adaptaci√≥n**: Evitan agua, cortan √°rboles cuando hay poca comida

---

## üß© Arquitectura Implementada

### ‚úÖ M√≥dulos Completados

- **`src/agents/`** - Sistema de agentes con red neuronal y aprendizaje puro
- **`src/world/`** - Mundo procedural con puzzle y recursos
- **`src/evolution/`** - Algoritmo gen√©tico completo (selecci√≥n, cruce, mutaci√≥n)
- **`src/analytics/`** - M√©tricas, clustering y monitoreo de aprendizaje
- **`src/ui/`** - Interfaz visual con pygame, estad√≠sticas y popups

### üîß Caracter√≠sticas Implementadas

- **Aprendizaje puro**: 100% aut√≥nomo, sin gu√≠a codificada
- **Red neuronal MLP**: 2 capas ocultas, 5 salidas (incluye acci√≥n `hit`)
- **Sistema de puzzle**: Fortalezas, llaves, puertas, cofre
- **Sistema de corte**: √Årboles y huts destructibles
- **Sistema de agua**: Penalizaciones proporcionales
- **Algoritmo gen√©tico**: Meeting pool, cruce uniforme, mutaci√≥n gaussiana
- **Clustering**: K-Means con PCA para identificar estrategias
- **Tiempo adaptativo**: Ticks por generaci√≥n aumentan progresivamente
- **Fitness natural**: Sin dependencia de generaci√≥n, basado en rendimiento real

### üìä Flujo del Sistema

1. **Inicializaci√≥n**: Carga `config.py`, genera mundo procedural, crea poblaci√≥n inicial
2. **Simulaci√≥n**: Loop con ticks, percepci√≥n ‚Üí decisi√≥n ‚Üí acci√≥n
3. **Fitness**: C√°lculo basado en rendimiento real (supervivencia, comida, exploraci√≥n, puzzle)
4. **Evoluci√≥n**: Algoritmo gen√©tico mejora poblaci√≥n
5. **An√°lisis**: Clustering y monitoreo de aprendizaje (cada 3 generaciones)
6. **Visualizaci√≥n**: Renderizado con pygame y HUD informativo (si no est√° en headless)

---

## üß≠ Principios de Dise√±o

El c√≥digo sigue estos principios:

- **Arquitectura Modular**: Separaci√≥n clara de responsabilidades
- **Aprendizaje Puro**: Sin gu√≠a codificada, agentes aprenden 100% aut√≥nomamente
- **Configuraci√≥n Centralizada**: Todo en `config.py` (SimulationConfig)
- **Reproducibilidad**: Semillas y configuraci√≥n determin√≠stica
- **Performance**: Optimizado para simulaciones largas
- **Extensibilidad**: F√°cil agregar nuevos elementos (obst√°culos, recompensas, etc.)

---

## üß© Estado del Proyecto

> **El proyecto `Ecosistema Evolutivo IA` est√° completamente implementado y funcional.**

‚úÖ **Todos los m√≥dulos est√°n implementados**‚úÖ **Arquitectura modular y extensible**‚úÖ **Sistema de configuraci√≥n centralizado**‚úÖ **Interfaz visual completa**‚úÖ **An√°lisis y m√©tricas**‚úÖ **Aprendizaje puro sin gu√≠a codificada**‚úÖ **Sistema de puzzle complejo**‚úÖ **Clustering de comportamientos emergentes**

> **El sistema est√° listo para ejecutar simulaciones y experimentos.**

---

## üìù Notas T√©cnicas

### Aprendizaje Puro

* Los agentes **NO reciben gu√≠a codificada** hacia comida, √°rboles o puzzle
* Todas las decisiones son tomadas por la red neuronal bas√°ndose en percepciones
* El fitness recompensa comportamientos exitosos, pero no "dirige" el comportamiento
* √önica excepci√≥n: Limitaci√≥n f√≠sica de giro excesivo para evitar c√≠rculos infinitos

### Sistema de Fitness

* **Sin dependencia de generaci√≥n**: El fitness no aumenta artificialmente con las generaciones
* **Basado en rendimiento real**: Supervivencia, comida, exploraci√≥n, eficiencia, puzzle
* **Penalizaciones proporcionales**: Agua penaliza seg√∫n tiempo transcurrido (sin l√≠mite)
* **Recompensas del puzzle**: Acumuladas durante la generaci√≥n, sumadas al fitness final

### Sistema Adaptativo de Tiempo

* **BASE_TICKS**: 600 ticks iniciales
* **Incremento**: +200 ticks cada 2 generaciones
* **Prop√≥sito**: Permitir m√°s tiempo para tareas complejas en generaciones avanzadas
* **Efecto**: Fitness puede aumentar naturalmente con m√°s tiempo disponible

---

## üéØ Objetivo del Puzzle

El objetivo principal del ecosistema es que los agentes aprendan a resolver el puzzle:

1. **Aprendizaje b√°sico** (Gen 1-4): Comer, sobrevivir, explorar
2. **Descubrimiento del puzzle** (Gen 5+): Aparecen llaves, agentes empiezan a interactuar
3. **Resoluci√≥n completa**: Agentes aprenden secuencia completa (llave ‚Üí puerta ‚Üí cofre)
4. **Fitness final**: Cuando abren el cofre, fitness promedio debe ser ‚â•60

La simulaci√≥n termina cuando el cofre es abierto, independientemente de la generaci√≥n.

---

## üìö Referencias y Conceptos

- **Algoritmos Gen√©ticos**: Selecci√≥n, cruce, mutaci√≥n, elitismo
- **Redes Neuronales**: MLP, forward pass, backpropagation (impl√≠cito v√≠a GA)
- **Clustering**: K-Means, PCA, normalizaci√≥n
- **Aprendizaje por Refuerzo Evolutivo**: Fitness como se√±al de recompensa
- **Emergencia**: Comportamientos complejos emergentes de reglas simples

---

**Desarrollado para el curso de Inteligencia Artificial - UCEMA**
